{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN\n",
    "\n",
    "- 영화 리뷰 감정 분석과 기계번역시에 단어의 특징만 잡는 것이 아닌, 순서가 주는 정보까지 인지하는 형태의 신경망 재현\n",
    "\n",
    "- 기존의 CNN 등의 모델은 사진 같은 정적 데이터를 인지한다. 현실에서는 순차적으로 경험하는 동적 데이터가 있다.\n",
    "- 데이터가 순서대로 나열된 데이터( 순차적 데이터, 시계열 데이터)의 정보를 받아 전체 내용을 학습하는 RNN\n",
    "- RNN은 정해지지 않은 길이의 배열을 읽고 설명하는 신경망.\n",
    "- RNN은 시계열 데이터 정보를 하나씩 이력 받을 때마다 지금까지 입력된 벡터들을 종합해 은닉 벡터를 만들어 낸다. \n",
    "\n",
    "  ( 마지막으로 만들어진 은틱벡터K는 배열 속 모든 벡터들의 내용을 압축한 벡터라고 할 수 있다. )\n",
    " \n",
    "- RNN계열의 신경망은 대표적으로 텍스트와 자연어를 처리하고 학습하는데 주로 이용. ( LSTM, GRU의 응용 RNN 개발 ) ->감정분석, 언어모델링, 기계번역에 이용\n",
    "- 데이터 순서 정보 학습하는 점에서 RNN은 CIFAR-10같은 정적 데이터보다는 동영상, 자연어, 주가 등 동적 데이터 이용시 성능 극대화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습셋 : 20000  // 검증셋 : 5000  //  테스트셋 : 25000 //  단어수 : 46159  // 클래스 : 2\n"
     ]
    }
   ],
   "source": [
    "#RNN을 이용해 텍스트 감정 분석 ( 영화 리뷰 감정 분석)\n",
    "#IMDB영화리뷰 데이터 이용(5만건) 부정리뷰 1, 긍정리뷰 2로 레이블링\n",
    "#리뷰의 긍정여부 판단하는 분류 모델 생성\n",
    "\n",
    "#라이버르리 임포트\n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "\n",
    "#하이퍼파라미터 정의\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 40\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "#데이터셋 로드, 텐서 변환\n",
    "\n",
    "#텍스트->텐서변환 설정\n",
    "TEXT = data.Field(sequential = True, batch_first = True, lower = True) #시퀀셜은 순차적인 데이터인지? 배치는 텐서 첫 입력차원 값이 배치사이즈인지?, lower로 모든 알파벳 소문자로 \n",
    "LABEL = data.Field(sequential = False, batch_first = True)\n",
    "\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "#임베딩용 단어사전 구축\n",
    "TEXT.build_vocab(trainset, min_freq = 5) #최소 5번 이상 등장한 단어만 사전에 포함 // 5번 미만은 unk 토큰으로 대체 \n",
    "LABEL.build_vocab(trainset)\n",
    "\n",
    "#검증 데이터셋의 부족으로 학습셋을 스플릿 하여 이용 (+배치 단위로 쪼개서 학습 진행)\n",
    "#데이터 스플릿\n",
    "trainset, valset = trainset.split(split_ratio = 0.8)\n",
    "#반복시 배치 생성 반복자 생성(iterator)\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((trainset, valset, testset), batch_size = BATCH_SIZE, shuffle=True, repeat = False)\n",
    "\n",
    "#사전속 단어, 레이블 수 정하는 변수 생성\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "\n",
    "#확인\n",
    "print('학습셋 : {}  // 검증셋 : {}  //  테스트셋 : {} //  단어수 : {}  // 클래스 : {}'.format(len(trainset), len(valset), len(testset), vocab_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 생성 BasicGRU\n",
    "\n",
    "class BasicGRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        print('Building Basic GRU model')\n",
    "        #은닉 벡터의 층은 복잡한 모델이 아닌 이상 2이하로 정의하는게 대부분\n",
    "        self.n_layers = n_layers\n",
    "        #임베딩은 모든 단어 수, 임베딩된 단어 텐서가 지니는 차원값을 입력받는다\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        #은닉 벡터 차원값, 드롭아웃 정의\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        #RNN모델 정의 RNN의 응용모델인 GRU를 사용하는 이유는 RNN은 입력이 길어지면 학습 도중 기울기가 너무 작거나 커져 앞부분의 정보를 정확히 담지 못할 수 있다.\n",
    "        #이를 경사도 폭발, 소실이라 하며 이를 보완하기위한 대표적인 것이 GRU이다. \n",
    "        #GRU는 시계열 데이터 속 벡터 사이의 정보 전달량을 조절함으로써 기울기를 적정하게 유지하고 문장 앞부분의 정보가 끝까지 도달할 수 있게 도와준다. \n",
    "        #GRU는 업데이트 게이트(이전 은닉벡터가 지닌 정보를 새로운 은닉벡터가 얼마나 유지할지 결정), 리셋게이트(새로운 입력이 이전 은닉벡터와 어떻게 조합하는지 결정)가 있다. \n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim, num_layers = self.n_layers, batch_first = True)\n",
    "        #문장 전체 맥락을 담은 텐서를 신경망에 통과시켜 클래스에 대한 예측 출력 \n",
    "        self.out= nn.Linear(self.hidden_dim, n_classes)\n",
    "        \n",
    "    #forward 함수 구현\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        #보통신경망과 달리 은닉벡터를 정의해 x와 함께 입력해줘야한다. \n",
    "        h_0 = self._init_state(batch_size = x.size(0))#은닉벡터\n",
    "        x, _ = self.gru(x, h_0) #은닉벡터의 시계열 배열 형태 반환 -> 배치사이즈, 입력 x길이, hidden_dim의 모양을 지닌 3d텐서\n",
    "        h_t = x[:,-1,:]#배치 내 모든 시계열 은닉 벡터들의 마지막 토큰들을 내포한 (batch_size, 1, hidden_dim)모양의 텐서 추출 // 영화 리뷰 배열 압축 은닉벡터\n",
    "        self.dropout(h_t) # 드롭아웃 설정\n",
    "        logit = self.out(h_t) #신경망에 입력하여 결과 출력\n",
    "        return logit\n",
    "    \n",
    "    # 파라미터 함수가 신경망 모듈의 가중치 정모를 반복자 형태로 반환 ( 원소들을 각 실제 신경망의 가중치 텐서를 지닌 객체)\n",
    "    def  _init_state(self, batch_size = 1):\n",
    "        weight = next(self.parameters()).data # 첫 가중치 텐서 추출\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() #모델 가중치와 같은 모양으로 텐서를 변환 후, 제로 호출해 텐서 내 모든값 0으로 초기화\n",
    "    #첫 은닉 벡터(h_0)는 보통 모든 특성값이 0인 벡터로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습, 평가 구현\n",
    "\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    #반복바다 배치 데이터 반환 , 영화평 데이터와 그에 상응하는 레이블은 batch.text, batch.label을 통해 접근가능\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1) #1과 2의 레이블값을 0과 1로 변환\n",
    "        optimizer.zero_grad() #매번 기울기를 새로 계산하여 기울기를 0으로 초기화\n",
    "        logit = model(x)\n",
    "        \n",
    "        #예측과 실제 레이블간의 오차를 구하고 기울기 계싼 후 최적화 과정의 반복\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter :\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1) #레이블 변환\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction = 'sum') # 각 배치의 평균이 아닌 데이터셋 전체 오차의 합\n",
    "        total_loss += loss.item() #오차합 누적\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum() #모델이 맞힌 수를 담는다.\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss/size #오차합과 corrects변수를 데이터셋 크기로 나누어 오차 평군과, 정확도 평균 반환\n",
    "    avg_accuracy = 100.0 *corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Basic GRU model\n"
     ]
    }
   ],
   "source": [
    "#모델 객체 정의 \n",
    "#은닉벡터 차원값 256  // 임베딩 토큰의 차원값 128로 임의 설정\n",
    "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
    "#최적화 알고리즘은 속도가 빠른 Adam이용\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCSH : 1  // Val_loss : 0.6924577838897705  //  val_accuracy : 52.34000015258789%\n",
      "EPOCSH : 2  // Val_loss : 0.6896089431762695  //  val_accuracy : 53.7400016784668%\n",
      "EPOCSH : 3  // Val_loss : 0.6723749813079833  //  val_accuracy : 58.439998626708984%\n",
      "EPOCSH : 4  // Val_loss : 0.3665471423625946  //  val_accuracy : 85.19999694824219%\n",
      "EPOCSH : 5  // Val_loss : 0.36541864686012265  //  val_accuracy : 85.45999908447266%\n",
      "EPOCSH : 6  // Val_loss : 0.3321242066502571  //  val_accuracy : 87.45999908447266%\n",
      "EPOCSH : 7  // Val_loss : 0.35852700452804565  //  val_accuracy : 87.05999755859375%\n",
      "EPOCSH : 8  // Val_loss : 0.38771652791500094  //  val_accuracy : 86.9800033569336%\n",
      "EPOCSH : 9  // Val_loss : 0.3848258887052536  //  val_accuracy : 87.23999786376953%\n",
      "EPOCSH : 10  // Val_loss : 0.4082783898353577  //  val_accuracy : 87.08000183105469%\n",
      "EPOCSH : 11  // Val_loss : 0.44137678174972533  //  val_accuracy : 86.5%\n",
      "EPOCSH : 12  // Val_loss : 0.46287915616035463  //  val_accuracy : 87.19999694824219%\n",
      "EPOCSH : 13  // Val_loss : 0.42794620575904846  //  val_accuracy : 87.04000091552734%\n",
      "EPOCSH : 14  // Val_loss : 0.45618103818893435  //  val_accuracy : 87.62000274658203%\n",
      "EPOCSH : 15  // Val_loss : 0.44987372217178345  //  val_accuracy : 87.5999984741211%\n",
      "EPOCSH : 16  // Val_loss : 0.47069825115203856  //  val_accuracy : 87.26000213623047%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3ed1a19210d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-05e85cbbed02>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#1과 2의 레이블값을 0과 1로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#매번 기울기를 새로 계산하여 기울기를 0으로 초기화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mlogit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#예측과 실제 레이블간의 오차를 구하고 기울기 계싼 후 최적화 과정의 반복\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-50a22c8b3d02>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#보통신경망과 달리 은닉벡터를 정의해 x와 함께 입력해줘야한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mh_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#은닉벡터\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#은닉벡터의 시계열 배열 형태 반환 -> 배치사이즈, 입력 x길이, hidden_dim의 모양을 지닌 3d텐서\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mh_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#배치 내 모든 시계열 은닉 벡터들의 마지막 토큰들을 내포한 (batch_size, 1, hidden_dim)모양의 텐서 추출 // 영화 리뷰 배열 압축 은닉벡터\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 드롭아웃 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    716\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#학습실행 \n",
    "best_val_loss = None\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "    \n",
    "    print(\"EPOCSH : {}  // Val_loss : {}  //  val_accuracy : {}%\".format(e, val_loss, val_accuracy))\n",
    "    \n",
    "    #검증오차가 최소인 최적모델 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir('snapshot'):\n",
    "            os.makedirs('snapshot')\n",
    "        torch.save(model.state_dict(), './snapshot/txtxclassification.pt')\n",
    "        best_val_loss = val_loss\n",
    "        \n",
    "#본인은 GPU가 지원이 안되 CPU로 매우 오랜시간이 걸린다. \n",
    "#GPU(CUDA)를 이용가능한지 그래픽카드 제조사 홈페이지에서 확인 후 실행하는 것을 권장\n",
    "#CPU 분석환경이 속도가 너무 느려 16epoch까지 진행후 학습 중단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 오차 : 0.3235560657119751  // 테스트 정확도 : 86.63200378417969\n"
     ]
    }
   ],
   "source": [
    "#학습 마친 후, 테스트셋으로 모델 성능 시험(검증 오차가 최소화된 모델)\n",
    "model.load_state_dict(torch.load('./snapshot/txtxclassification.pt'))\n",
    "test_loss , test_acc = evaluate(model, test_iter)\n",
    "print('테스트 오차 : {}  // 테스트 정확도 : {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
